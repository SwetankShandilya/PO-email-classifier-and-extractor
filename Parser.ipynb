{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting Email Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imapclient\n",
    "import email\n",
    "from email import policy\n",
    "from email.header import decode_header\n",
    "import os\n",
    "import pdfplumber\n",
    "from PIL import Image\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imapclient import IMAPClient\n",
    "def connect_to_email(username, password, imap_server):\n",
    "    client = imapclient.IMAPClient(\"imap.gmail.com\", ssl=True)\n",
    "    client.login(username, password)\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from email import message_from_bytes\n",
    "from email.policy import default as email_policy\n",
    "\n",
    "def fetch_emails(client, folder=\"INBOX\", max_emails=10):\n",
    "    # Select the folder (e.g., \"INBOX\")\n",
    "    client.select_folder(folder)\n",
    "    \n",
    "    # Fetch only unread emails in the Primary inbox\n",
    "    messages = client.search(['X-GM-RAW', 'category:primary is:unread'])\n",
    "    \n",
    "    # Sort messages by UID in descending order to process the latest emails first\n",
    "    messages = sorted(messages, reverse=True)\n",
    "    \n",
    "    # Process emails up to the specified max_emails limit\n",
    "    processed_count = 0\n",
    "    for uid in messages:\n",
    "        if processed_count >= max_emails:  # Stop once the limit is reached\n",
    "            break\n",
    "        raw_message = client.fetch(uid, \"RFC822\")[uid][b\"RFC822\"]\n",
    "        message = message_from_bytes(raw_message, policy=email_policy)\n",
    "        processed_count += 1\n",
    "        yield message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_email_text(email_body):\n",
    "    cleaned_text = re.sub(r'\\[Attachment: .*?\\]', '', email_body)\n",
    "    cleaned_text = ' '.join(cleaned_text.split())\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process each email and extract subject, body, and attachments\n",
    "def process_email(message):\n",
    "    subject = decode_header(message[\"subject\"])[0][0]\n",
    "    if isinstance(subject, bytes):\n",
    "        subject = subject.decode()\n",
    "\n",
    "    body = \"\"\n",
    "    attachments = []\n",
    "    has_attachments=False\n",
    "    # Extract email body and attachments\n",
    "    if message.is_multipart():\n",
    "        for part in message.iter_parts():\n",
    "            content_type = part.get_content_type()\n",
    "            content_disposition = str(part.get(\"Content-Disposition\"))\n",
    "\n",
    "            # If it's a text part, extract the body\n",
    "            if \"text\" in content_type:\n",
    "                body += part.get_payload(decode=True).decode()\n",
    "            # If it's an attachment, save it\n",
    "            if \"attachment\" in content_disposition:\n",
    "                has_attachments = True\n",
    "                filename = part.get_filename()\n",
    "                attachment_data = part.get_payload(decode=True)\n",
    "                attachments.append((filename, attachment_data))\n",
    "    else:\n",
    "        body = message.get_payload(decode=True).decode()\n",
    "\n",
    "   \n",
    "\n",
    "    body= clean_email_text(body)\n",
    "    subject=clean_email_text(subject)\n",
    "    return subject, body, attachments, has_attachments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(subject, body, has_attachment):\n",
    "    attachment_feature = f\" [ATTACHMENT_SIGNAL: {'PRESENT' if has_attachment else 'ABSENT'}]\"\n",
    "    combined_text = f\"Subject Prefix: {'PO' if 'purchase order' in subject.lower() else 'GENERIC'} Subject: {subject} Body: {body}{attachment_feature}\"\n",
    "    return combined_text\n",
    "\n",
    "def tokenize_input(text, tokenizer):\n",
    "    return tokenizer(text, max_length=512, truncation=True, padding=\"max_length\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparaing Data and Model Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "import torch\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_csv(csv_file):\n",
    "    \n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    df = df.dropna(how='all')\n",
    "    \n",
    "    df['has_attachment'] = False  \n",
    "\n",
    "    df.loc[df['label'] == 'PO', 'has_attachment'] = True\n",
    "    \n",
    "    non_po_indices = df[df['label'] == 'Non-PO'].index\n",
    "    non_po_true_indices = df[df['label'] == 'Non-PO'].sample(frac=0.2, random_state=42).index\n",
    "    df.loc[non_po_true_indices, 'has_attachment'] = True\n",
    "\n",
    "    \n",
    "    \n",
    "    texts = df['text'].tolist()  \n",
    "    labels = df['label'].tolist()\n",
    "    flags=df['has_attachment'].tolist()\n",
    "    \n",
    "    return texts, labels, flags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class EmailDataset(Dataset):\n",
    "    def __init__(self, texts, labels, flags, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.flags = flags \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        flag = self.flags[idx]\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            text, \n",
    "            truncation=True, \n",
    "            padding='max_length', \n",
    "            max_length=self.max_length, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        inputs = {key: val.squeeze(0) for key, val in inputs.items()}\n",
    "        \n",
    "        return {\n",
    "            **inputs, \n",
    "            'labels': torch.tensor(label, dtype=torch.long),  # Convert label to tensor\n",
    "            'flags': torch.tensor(flag, dtype=torch.bool)  # Convert flag to tensor\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv=\"E:\\Projects\\GenAI\\AI-Powered Automated PO Tool\\AI-Powered-Purchase-Order-Parser\\Train Data.csv\"\n",
    "test_csv=\"E:\\Projects\\GenAI\\AI-Powered Automated PO Tool\\AI-Powered-Purchase-Order-Parser\\Test Data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, train_labels, train_flags = load_data_from_csv(train_csv)\n",
    "test_texts, test_labels, test_flags = load_data_from_csv(test_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\"PO\": 1, \"Non-PO\": 0}  # Example mapping\n",
    "train_labels = [label_mapping[label] for label in train_labels]\n",
    "test_labels = [label_mapping[label] for label in test_labels]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EmailDataset(train_texts, train_labels, train_flags, tokenizer)\n",
    "test_dataset = EmailDataset(test_texts, test_labels, test_flags, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sweta\\anaconda3\\envs\\POP\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    save_total_limit=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer = Trainer(\n",
    "#    model=model,\n",
    "#    args=training_args,\n",
    "#   train_dataset=train_dataset,\n",
    "#    eval_dataset=test_dataset\n",
    "#)\n",
    "\n",
    "#trainer.train()\n",
    "\n",
    "#model.save_pretrained('./po_email_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "model_path = \"./po_email_model\" \n",
    "usable_model = AutoModelForSequenceClassification.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def classify_text(text):\\n    # Tokenize the input text\\n    encoding = tokenizer.encode_plus(\\n        text,\\n        add_special_tokens=True,\\n        max_length=128,\\n        padding=\\'max_length\\',\\n        truncation=True,\\n        return_tensors=\\'pt\\'\\n    )\\n\\n    # Get input ids and attention mask\\n    input_ids = encoding[\\'input_ids\\']\\n    attention_mask = encoding[\\'attention_mask\\']\\n\\n    # Predict using the model\\n    with torch.no_grad():\\n        outputs = usable_model(input_ids=input_ids, attention_mask=attention_mask)\\n        logits = outputs.logits\\n        prediction = torch.argmax(logits, dim=1).item()  # Get the predicted class\\n\\n    return prediction\\n\\n# Get custom input text\\ninput_text = input(\"Enter the text to classify: \")\\n\\n# Classify the input text\\npredicted_class = classify_text(input_text)\\nprint(f\"Predicted Class: {predicted_class}\")\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def classify_text(text):\n",
    "    # Tokenize the input text\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    # Get input ids and attention mask\n",
    "    input_ids = encoding['input_ids']\n",
    "    attention_mask = encoding['attention_mask']\n",
    "\n",
    "    # Predict using the model\n",
    "    with torch.no_grad():\n",
    "        outputs = usable_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        prediction = torch.argmax(logits, dim=1).item()  # Get the predicted class\n",
    "\n",
    "    return prediction\n",
    "\n",
    "# Get custom input text\n",
    "input_text = input(\"Enter the text to classify: \")\n",
    "\n",
    "# Classify the input text\n",
    "predicted_class = classify_text(input_text)\n",
    "print(f\"Predicted Class: {predicted_class}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Email Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_email(text, usable_model, tokenizer):\n",
    "    inputs = tokenize_input(text, tokenizer)\n",
    "    outputs = usable_model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    prediction = torch.argmax(logits, dim=1).item()\n",
    "    return \"PO Email\" if prediction == 1 else \"Non-PO Email\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import os\n",
    "\n",
    "def extract_text_from_pdf(pdf_content):\n",
    "\n",
    "    temp_file = \"temp.pdf\"\n",
    "    try:\n",
    "        \n",
    "        with open(temp_file, \"wb\") as f:\n",
    "            f.write(pdf_content)\n",
    "        \n",
    "        with pdfplumber.open(temp_file) as pdf:\n",
    "            text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                text += page.extract_text() or \"\"  \n",
    "        \n",
    "\n",
    "        lines = text.split('\\n')\n",
    "        \n",
    "        \n",
    "        if _is_table_like(lines):\n",
    "            return _format_as_table(lines)\n",
    "        else:\n",
    "            return '\\n'.join(lines)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF: {e}\")\n",
    "        return None\n",
    "    \n",
    "    finally:\n",
    "        \n",
    "        if os.path.exists(temp_file):\n",
    "            os.remove(temp_file)\n",
    "\n",
    "\n",
    "def _is_table_like(lines):\n",
    "    \n",
    "    return all('\\t' in line for line in lines)\n",
    "\n",
    "def _format_as_table(lines):\n",
    "\n",
    "    return [line.split('\\t') for line in lines]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easyocr\n",
    "from PIL import Image\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "def extract_text_from_image(image_content):\n",
    "    # Load the image from byte content\n",
    "    image = Image.open(io.BytesIO(image_content))\n",
    "    \n",
    "    # Initialize EasyOCR Reader\n",
    "    reader = easyocr.Reader(['en'])  # Supports multiple languages, 'en' for English\n",
    "    \n",
    "    # Perform OCR and get the result\n",
    "    result = reader.readtext(image)\n",
    "    \n",
    "    # Sort results by vertical and horizontal position\n",
    "    result.sort(key=lambda x: (x[0][0][1], x[0][0][0]))\n",
    "    \n",
    "    # Group texts into lines and detect table-like structures\n",
    "    lines = []\n",
    "    current_line = []\n",
    "    current_y = result[0][0][0][1]\n",
    "    \n",
    "    for item in result:\n",
    "        text = item[1]\n",
    "        top_left = item[0][0]\n",
    "        \n",
    "        # Check if the item is in the same line (within a small vertical threshold)\n",
    "        if abs(top_left[1] - current_y) < 20:\n",
    "            current_line.append((top_left[0], text))\n",
    "        else:\n",
    "            # Sort the current line by horizontal position\n",
    "            current_line.sort(key=lambda x: x[0])\n",
    "            lines.append(' '.join(item[1] for item in current_line))\n",
    "            current_line = [(top_left[0], text)]\n",
    "            current_y = top_left[1]\n",
    "    \n",
    "    # Add the last line\n",
    "    if current_line:\n",
    "        current_line.sort(key=lambda x: x[0])\n",
    "        lines.append(' '.join(item[1] for item in current_line))\n",
    "    \n",
    "    # Detect if the extracted text resembles a table\n",
    "    if _is_table_like(lines):\n",
    "        return _format_as_table(lines)\n",
    "    else:\n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "def _is_table_like(lines):\n",
    "    # Heuristics to detect if the text looks like a table\n",
    "    delimiter_count = sum(1 for line in lines if '|' in line or '\\t' in line)\n",
    "    return delimiter_count > len(lines) * 0.5\n",
    "\n",
    "def _format_as_table(lines):\n",
    "    # Try to create a more structured table representation\n",
    "    table_lines = []\n",
    "    for line in lines:\n",
    "        # Split by multiple spaces or tabs\n",
    "        parts = [p.strip() for p in line.replace('|', '\\t').split('\\t') if p.strip()]\n",
    "        table_lines.append('\\t'.join(parts))\n",
    "    \n",
    "    return '\\n'.join(table_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_classification_pipeline(username, password, imap_server, usable_model, tokenizer):\n",
    "    client = connect_to_email(username, password, imap_server)\n",
    "    \n",
    "    for message in fetch_emails(client):\n",
    "        \n",
    "        subject, body, email_attachments, attachment_flag = process_email(message)\n",
    "        \n",
    "        combined_text = prepare_input(subject, body, attachment_flag)\n",
    "        \n",
    "        result = classify_email(combined_text, usable_model, tokenizer)\n",
    "        \n",
    "        print(f\"Email classified as: {result}\")\n",
    "        \n",
    "        if result == 'PO Email':  \n",
    "            for filename, attachment in email_attachments:\n",
    "\n",
    "                if filename.lower().endswith(\".pdf\"):\n",
    "                    print(f\"Processing PDF attachment: {filename}\")\n",
    "                    order_details = extract_text_from_pdf(attachment)\n",
    "                    print(f\"Extracted order details: {order_details}\")\n",
    "                \n",
    "                elif filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                    print(f\"Processing image attachment: {filename}\")\n",
    "                    order_details = extract_text_from_image(attachment)\n",
    "                    print(f\"Extracted order details: {order_details}\")\n",
    "        else:\n",
    "            print(f\"Skipping attachment processing for non-PO email: {subject}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email classified as: Non-PO Email\n",
      "Processing PDF attachment: pdf PO.pdf\n",
      "Extracted order details: PURCHASE ORDER\n",
      "[Company Name]\n",
      "[Street Address] DATE 9/29/2015\n",
      "[City, ST ZIP] PO # [123456]\n",
      "Phone: (000) 000-0000\n",
      "Fax: (000) 000-0000\n",
      "Website:\n",
      "VENDOR SHIP TO\n",
      "[Company Name] [Name]\n",
      "[Contact or Department] [Company Name]\n",
      "[Street Address] [Street Address]\n",
      "[City, ST ZIP] [City, ST ZIP]\n",
      "Phone: (000) 000-0000 [Phone]\n",
      "Fax: (000) 000-0000\n",
      "REQUISITIONER SHIP VIA F.O.B. SHIPPING TERMS\n",
      "ITEM # DESCRIPTION QTY UNIT PRICE TOTAL\n",
      "[23423423] Product XYZ 15 150.00 2,250.00\n",
      "[45645645] Product ABC 1 75.00 75.00\n",
      "SUBTOTAL 2,325.00\n",
      "Comments or Special Instructions TAX\n",
      "Thank you for your business. SHIPPING\n",
      "OTHER\n",
      "TOTAL $2,325.00\n",
      "If you have any questions about this purchase order,please contact\n",
      "https://www.vertex42.com/ExcelTemplates/excel-purchase-order.html Purchase Order Template Â© 2015 Vertex42.com[Name, Phone #, E-mail]\n",
      "https://www.vertex42.com/ExcelTemplates/excel-purchase-order.html Purchase Order Template Â© 2015 Vertex42.com\n",
      "Email classified as: Non-PO Email\n",
      "Email classified as: Non-PO Email\n",
      "Email classified as: Non-PO Email\n",
      "Email classified as: Non-PO Email\n",
      "Email classified as: Non-PO Email\n",
      "Email classified as: Non-PO Email\n",
      "Email classified as: Non-PO Email\n",
      "Email classified as: Non-PO Email\n",
      "Email classified as: Non-PO Email\n"
     ]
    }
   ],
   "source": [
    "email_classification_pipeline(\"you_email_id@gmail.com\", \"app_password\", \"imap.gmail.com\", usable_model, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "POP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
