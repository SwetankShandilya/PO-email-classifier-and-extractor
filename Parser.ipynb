{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting Email Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imapclient\n",
    "import email\n",
    "from email import policy\n",
    "from email.header import decode_header\n",
    "import os\n",
    "import pdfplumber\n",
    "from PIL import Image\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imapclient import IMAPClient\n",
    "def connect_to_email(username, password, imap_server):\n",
    "    client = imapclient.IMAPClient(\"imap.gmail.com\", ssl=True)\n",
    "    client.login(username, password)\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from email import message_from_bytes\n",
    "from email.policy import default as email_policy\n",
    "\n",
    "def fetch_emails(client, folder=\"INBOX\", max_emails=10):\n",
    " \n",
    "    client.select_folder(folder)\n",
    "    \n",
    "    # Fetch only unread emails in the Primary inbox\n",
    "    messages = client.search(['X-GM-RAW', 'category:primary is:unread'])\n",
    "    \n",
    "    # Sort messages by UID in descending order to process the latest emails first\n",
    "    messages = sorted(messages, reverse=True)\n",
    "    \n",
    "  \n",
    "    processed_count = 0\n",
    "    for uid in messages:\n",
    "        if processed_count >= max_emails:  # Stop once the limit is reached\n",
    "            break\n",
    "        raw_message = client.fetch(uid, \"RFC822\")[uid][b\"RFC822\"]\n",
    "        message = message_from_bytes(raw_message, policy=email_policy)\n",
    "        processed_count += 1\n",
    "        yield message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_email_text(email_body):\n",
    "    cleaned_text = re.sub(r'\\[Attachment: .*?\\]', '', email_body)\n",
    "    cleaned_text = ' '.join(cleaned_text.split())\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process each email and extract subject, body, and attachments\n",
    "def process_email(message):\n",
    "    subject = decode_header(message[\"subject\"])[0][0]\n",
    "    if isinstance(subject, bytes):\n",
    "        subject = subject.decode()\n",
    "\n",
    "    body = \"\"\n",
    "    attachments = []\n",
    "    has_attachments=False\n",
    "    \n",
    "    if message.is_multipart():\n",
    "        for part in message.iter_parts():\n",
    "            content_type = part.get_content_type()\n",
    "            content_disposition = str(part.get(\"Content-Disposition\"))\n",
    "\n",
    "           \n",
    "            if \"text\" in content_type:\n",
    "                body += part.get_payload(decode=True).decode()\n",
    "          \n",
    "            if \"attachment\" in content_disposition:\n",
    "                has_attachments = True\n",
    "                filename = part.get_filename()\n",
    "                attachment_data = part.get_payload(decode=True)\n",
    "                attachments.append((filename, attachment_data))\n",
    "    else:\n",
    "        body = message.get_payload(decode=True).decode()\n",
    "\n",
    "   \n",
    "\n",
    "    body= clean_email_text(body)\n",
    "    subject=clean_email_text(subject)\n",
    "    return subject, body, attachments, has_attachments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(subject, body, has_attachment):\n",
    "    attachment_feature = f\" [ATTACHMENT_SIGNAL: {'PRESENT' if has_attachment else 'ABSENT'}]\"\n",
    "    combined_text = f\"Subject Prefix: {'PO' if 'purchase order' in subject.lower() else 'GENERIC'} Subject: {subject} Body: {body}{attachment_feature}\"\n",
    "    return combined_text\n",
    "\n",
    "def tokenize_input(text, tokenizer):\n",
    "    return tokenizer(text, max_length=512, truncation=True, padding=\"max_length\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparaing Data and Model Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "import torch\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_csv(csv_file):\n",
    "    \n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    df = df.dropna(how='all')\n",
    "    \n",
    "    df['has_attachment'] = False  \n",
    "\n",
    "    df.loc[df['label'] == 'PO', 'has_attachment'] = True\n",
    "    \n",
    "    non_po_indices = df[df['label'] == 'Non-PO'].index\n",
    "    non_po_true_indices = df[df['label'] == 'Non-PO'].sample(frac=0.2, random_state=42).index\n",
    "    df.loc[non_po_true_indices, 'has_attachment'] = True\n",
    "\n",
    "    \n",
    "    \n",
    "    texts = df['text'].tolist()  \n",
    "    labels = df['label'].tolist()\n",
    "    flags=df['has_attachment'].tolist()\n",
    "    \n",
    "    return texts, labels, flags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class EmailDataset(Dataset):\n",
    "    def __init__(self, texts, labels, flags, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.flags = flags \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        flag = self.flags[idx]\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            text, \n",
    "            truncation=True, \n",
    "            padding='max_length', \n",
    "            max_length=self.max_length, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        inputs = {key: val.squeeze(0) for key, val in inputs.items()}\n",
    "        \n",
    "        return {\n",
    "            **inputs, \n",
    "            'labels': torch.tensor(label, dtype=torch.long),  # Convert label to tensor\n",
    "            'flags': torch.tensor(flag, dtype=torch.bool)  # Convert flag to tensor\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv=\"E:\\Projects\\GenAI\\AI-Powered Automated PO Tool\\AI-Powered-Purchase-Order-Parser\\Train Data.csv\"\n",
    "test_csv=\"E:\\Projects\\GenAI\\AI-Powered Automated PO Tool\\AI-Powered-Purchase-Order-Parser\\Test Data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, train_labels, train_flags = load_data_from_csv(train_csv)\n",
    "test_texts, test_labels, test_flags = load_data_from_csv(test_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\"PO\": 1, \"Non-PO\": 0}  \n",
    "train_labels = [label_mapping[label] for label in train_labels]\n",
    "test_labels = [label_mapping[label] for label in test_labels]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EmailDataset(train_texts, train_labels, train_flags, tokenizer)\n",
    "test_dataset = EmailDataset(test_texts, test_labels, test_flags, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    save_total_limit=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer = Trainer(\n",
    "#    model=model,\n",
    "#    args=training_args,\n",
    "#   train_dataset=train_dataset,\n",
    "#    eval_dataset=test_dataset\n",
    "#)\n",
    "\n",
    "#trainer.train()\n",
    "\n",
    "#model.save_pretrained('./po_email_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "model_path = \"./po_email_model\" \n",
    "usable_model = AutoModelForSequenceClassification.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check classifier\n",
    "\n",
    "\n",
    "# def classify_text(text):\n",
    "#     # Tokenize the input text\n",
    "#     encoding = tokenizer.encode_plus(\n",
    "#         text,\n",
    "#         add_special_tokens=True,\n",
    "#         max_length=128,\n",
    "#         padding='max_length',\n",
    "#         truncation=True,\n",
    "#         return_tensors='pt'\n",
    "#     )\n",
    "\n",
    "#     # Get input ids and attention mask\n",
    "#     input_ids = encoding['input_ids']\n",
    "#     attention_mask = encoding['attention_mask']\n",
    "\n",
    "#     # Predict using the model\n",
    "#     with torch.no_grad():\n",
    "#         outputs = usable_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         logits = outputs.logits\n",
    "#         prediction = torch.argmax(logits, dim=1).item()  # Get the predicted class\n",
    "\n",
    "#     return prediction\n",
    "\n",
    "# # Get custom input text\n",
    "# input_text = input(\"Enter the text to classify: \")\n",
    "\n",
    "# # Classify the input text\n",
    "# predicted_class = classify_text(input_text)\n",
    "# print(f\"Predicted Class: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Email Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_email(text, usable_model, tokenizer):\n",
    "    inputs = tokenize_input(text, tokenizer)\n",
    "    outputs = usable_model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    prediction = torch.argmax(logits, dim=1).item()\n",
    "    return \"PO Email\" if prediction == 1 else \"Non-PO Email\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import os\n",
    "\n",
    "def extract_text_from_pdf(pdf_content):\n",
    "\n",
    "    temp_file = \"temp.pdf\"\n",
    "    try:\n",
    "        \n",
    "        with open(temp_file, \"wb\") as f:\n",
    "            f.write(pdf_content)\n",
    "        \n",
    "        with pdfplumber.open(temp_file) as pdf:\n",
    "            text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                text += page.extract_text() or \"\"  \n",
    "        \n",
    "\n",
    "        lines = text.split('\\n')\n",
    "        \n",
    "        \n",
    "        if _is_table_like(lines):\n",
    "            return _format_as_table(lines)\n",
    "        else:\n",
    "            return '\\n'.join(lines)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF: {e}\")\n",
    "        return None\n",
    "    \n",
    "    finally:\n",
    "        \n",
    "        if os.path.exists(temp_file):\n",
    "            os.remove(temp_file)\n",
    "\n",
    "\n",
    "def _is_table_like(lines):\n",
    "    \n",
    "    return all('\\t' in line for line in lines)\n",
    "\n",
    "def _format_as_table(lines):\n",
    "\n",
    "    return [line.split('\\t') for line in lines]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easyocr\n",
    "from PIL import Image\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "def extract_text_from_image(image_content):\n",
    "    \n",
    "    image = Image.open(io.BytesIO(image_content))\n",
    "\n",
    "    \n",
    "    reader = easyocr.Reader(['en'])  # Supports multiple languages, 'en' for English\n",
    "    \n",
    "    \n",
    "    result = reader.readtext(image)\n",
    "    \n",
    "    \n",
    "    result.sort(key=lambda x: (x[0][0][1], x[0][0][0]))\n",
    "    \n",
    "    \n",
    "    lines = []\n",
    "    current_line = []\n",
    "    current_y = result[0][0][0][1]\n",
    "    \n",
    "    for item in result:\n",
    "        text = item[1]\n",
    "        top_left = item[0][0]\n",
    "        \n",
    "        \n",
    "        if abs(top_left[1] - current_y) < 20:\n",
    "            current_line.append((top_left[0], text))\n",
    "        else:\n",
    "            \n",
    "            current_line.sort(key=lambda x: x[0])\n",
    "            lines.append(' '.join(item[1] for item in current_line))\n",
    "            current_line = [(top_left[0], text)]\n",
    "            current_y = top_left[1]\n",
    "    \n",
    "   \n",
    "    if current_line:\n",
    "        current_line.sort(key=lambda x: x[0])\n",
    "        lines.append(' '.join(item[1] for item in current_line))\n",
    "    \n",
    "   \n",
    "    if _is_table_like(lines):\n",
    "        return _format_as_table(lines)\n",
    "    else:\n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "def _is_table_like(lines):\n",
    "   \n",
    "    delimiter_count = sum(1 for line in lines if '|' in line or '\\t' in line)\n",
    "    return delimiter_count > len(lines) * 0.5\n",
    "\n",
    "def _format_as_table(lines):\n",
    "    \n",
    "    table_lines = []\n",
    "    for line in lines:\n",
    "        \n",
    "        parts = [p.strip() for p in line.replace('|', '\\t').split('\\t') if p.strip()]\n",
    "        table_lines.append('\\t'.join(parts))\n",
    "    \n",
    "    return '\\n'.join(table_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_classification_pipeline(username, password, imap_server, usable_model, tokenizer):\n",
    "    client = connect_to_email(username, password, imap_server)\n",
    "    \n",
    "    for message in fetch_emails(client):\n",
    "        \n",
    "        subject, body, email_attachments, attachment_flag = process_email(message)\n",
    "        \n",
    "        combined_text = prepare_input(subject, body, attachment_flag)\n",
    "        \n",
    "        result = classify_email(combined_text, usable_model, tokenizer)\n",
    "        \n",
    "        print(f\"Email classified as: {result}\")\n",
    "        \n",
    "        if result == 'PO Email':  \n",
    "            for filename, attachment in email_attachments:\n",
    "\n",
    "                if filename.lower().endswith(\".pdf\"):\n",
    "                    print(f\"Processing PDF attachment: {filename}\")\n",
    "                    order_details = extract_text_from_pdf(attachment)\n",
    "                    print(f\"Extracted order details: {order_details}\")\n",
    "                \n",
    "                elif filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                    print(f\"Processing image attachment: {filename}\")\n",
    "                    order_details = extract_text_from_image(attachment)\n",
    "                    print(f\"Extracted order details: {order_details}\")\n",
    "        else:\n",
    "            print(f\"Skipping attachment processing for non-PO email: {subject}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_classification_pipeline(\"you_email_id@gmail.com\", \"app_password\", \"imap.gmail.com\", usable_model, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "POP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
